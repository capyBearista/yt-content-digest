# ==========================================
# LLM SETTINGS
# ==========================================
# Configure which LLM to use for generating the summary.
# The script uses LiteLLM, so it supports 100+ providers.
# Just uncomment the block for the provider you want to use.

# --- OPTION 1: OLLAMA (Local, Free) ---
# llm_provider: ollama
# llm_model: "llama3.2:3b"
# URL where Ollama is running. Use "http://host.docker.internal:11434" if running via Docker.
ollama_base_url: "http://localhost:11434"

# --- OPTION 2: OPENAI ---
# llm_provider: "openai"
# llm_model: "gpt-4.1-mini"

# --- OPTION 3: GOOGLE GEMINI ---
# llm_provider: "gemini"
# llm_model: "gemini-2.5-pro"

# --- OPTION 4: OPENROUTER (Access to huge variety of models) ---
llm_provider: "openrouter"
llm_model: "x-ai/grok-4.1-fast:free"

# --- OPTION 5: GROQ (Extremely fast inference) ---
# llm_provider: "groq"
# llm_model: "openai/gpt-oss-120b"

# -- OPTION 6: CEREBRAS --- (NOT IMPLEMENTED YET)
# llm_provider: "cerebras"
# llm_model: "cerebras/Cerebras-GPT-13B"


# ==========================================
# TRANSCRIPTION SETTINGS
# ==========================================
# Options: 
#   - "local" (Uses faster-whisper on your machine. FREE.)
#   - "openai" (Uses OpenAI API. PAID, high ACCURACY, REALLY SLOW.)
#   - "groq"   (Uses Groq API. PAID, high SPEED.)
transcription_provider: "local"

# [Local Only] Settings for faster-whisper
# Models: tiny, base, small, medium, large-v3
local_whisper_model: "base"
# Device: "auto", "cpu", "cuda" (use "cuda" if you have an NVIDIA GPU)
local_whisper_device: "auto"
# Compute Type: "int8" (best for CPU), "float16" (best for GPU)
local_whisper_compute_type: "int8"


# ==========================================
# API KEYS
# ==========================================
# Optional, you can leave blank if not using.
api_keys:
  OPENAI_API_KEY: ""
  ANTHROPIC_API_KEY: ""
  GEMINI_API_KEY: ""
  OPENROUTER_API_KEY: ""
  GROQ_API_KEY: ""
  CEREBRAS_API_KEY: ""